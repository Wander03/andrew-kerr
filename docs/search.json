[
  {
    "objectID": "thesis.html",
    "href": "thesis.html",
    "title": "Thesis",
    "section": "",
    "text": "For my thesis, I contributed new functionality to the tidyclust package in R. The tidyclust package exists as a part of the tidymodels framework , which follows the principles of the tidyverse to establish a consistent and reproducible workflow for unsupervised learning algorithms. My work adapted the Apriori and ECLAT algorithms for frequent itemset mining. This involved establishing a cluster assignment strategy that groups items based on frequent itemsets and support values, as well as a standardized methodology for predicting missing items. You can read more about my design choices in my paper or see them in my presentation!\nThesis Paper - Frequent Itemset Mining with tidyclust in R\nThesis Code - tidyclust Package\nThesis Presentation"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Graduate Consulting: Spinal Segment Angle Analysis\n\n\n\nConsulting\n\nR\n\nStatistics\n\n\n\nPractice in statistical consulting. Observing faculty-led consulting sessions. Organizing and leading consulting projects with faculty supervision. Discussion of statistical…\n\n\n\n\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Consulting: Statistical Analysis of Soil Data\n\n\n\nConsulting\n\nR\n\nStatistics\n\n\n\nPractice in statistical consulting. Observing faculty-led consulting sessions. Organizing and leading consulting projects with faculty supervision. Discussion of statistical…\n\n\n\n\n\n\nJun 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT 415: Bayesian Survival Analysis of Customer Churn\n\n\n\nR\n\nStatistics\n\n\n\nBayes’ theorem, prior and posterior distributions, likelihood functions, Markov Chain Monte Carlo methods, hierarchical modeling. Bayesian data analysis, comparison of…\n\n\n\n\n\n\nJun 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Consulting: Balance Detection using Phones\n\n\n\nConsulting\n\nJMP\n\nSAS\n\nStatistics\n\n\n\nPractice in statistical consulting. Observing faculty-led consulting sessions. Organizing and leading consulting projects with faculty supervision. Discussion of statistical…\n\n\n\n\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Consulting: Spinal Segment Curve Analysis\n\n\n\nConsulting\n\nPython\n\nR\n\nStatistics\n\n\n\nPractice in statistical consulting. Observing faculty-led consulting sessions. Organizing and leading consulting projects with faculty supervision. Discussion of statistical…\n\n\n\n\n\n\nMay 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT 417: Survival Analysis of Customer Churn\n\n\n\nMinitab\n\nR\n\nStatistics\n\n\n\nParametric and nonparametric methods for analyzing survival data. Topics include Kaplan-Meier and Nelson-Aalen estimates, Cox regression models, accelerated failure time…\n\n\n\n\n\n\nMar 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Capstone: World Bank Digital Development Classification Project\n\n\n\nConsulting\n\nData Science\n\nPython\n\nStatistics\n\n\n\nTeam-based design, implementation, deployment and delivery of a system or analytical methodology that involves working with and analyzing large quantities of data. Technical…\n\n\n\n\n\n\nJun 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT 550: Generalized Linear Models\n\n\n\nR\n\nStatistics\n\n\n\nTheory and application of linear and generalized linear models (GLMs). Logistic regression, nominal and ordinal responses, Poisson GLMs, correlated responses, random and…\n\n\n\n\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nData 403: Home Credit Default Risk\n\n\n\nData Science\n\nPython\n\nStatistics\n\n\n\nProject-based lab component of DATA 401 and DATA 402. Projects involving comparison of predictive and interpretable regression models, implementing linear classifiers with…\n\n\n\n\n\n\nNov 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData 403: Iowa Liquor Sales Analysis\n\n\n\nData Science\n\nPython\n\nStatistics\n\n\n\nProject-based lab component of DATA 401 and DATA 402. Projects involving comparison of predictive and interpretable regression models, implementing linear classifiers with…\n\n\n\n\n\n\nOct 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT 545: Applied Stochastic Processes\n\n\n\nR\n\nStatistics\n\n\n\nProperties, simulation, and application of stochastic processes. Discrete-time and continuous-time Markov chains, hidden Markov models, Poisson processes, Gaussian…\n\n\n\n\n\n\nJun 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSC 369: Electric Vehicle Ownership\n\n\n\nData Science\n\nHadoop\n\nJava\n\nPySpark\n\nPython\n\n\n\nIntroduction to distributed computing paradigms and cloud computing. Modern distributed computing infrastructures. Problem-solving in a distributed computing environment.\n\n\n\n\n\n\nMar 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSC 365: Esports Gambling Application\n\n\n\nPython\n\nSQL\n\n\n\nBasic principles of database management systems (DBMS) and of DBMS application development. DBMS objectives, systems architecture, database models with emphasis on…\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSC 203: Virtual World Project\n\n\n\nJava\n\n\n\nObject-oriented programming and design with applications to project construction. Introduction to class design, interfaces, inheritance, generics, exceptions, streams, and…\n\n\n\n\n\n\nMar 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nData 301: Essay Autograder\n\n\n\nData Science\n\nPython\n\n\n\nIntroduction to the field of data science and the workflow of a data scientist. Types of data (tabular, textual, sparse, structured, temporal, geospatial), basic data…\n\n\n\n\n\n\nMar 8, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2025-06-10-stat566-04/patrick_michelsen.html",
    "href": "projects/2025-06-10-stat566-04/patrick_michelsen.html",
    "title": "Graduate Consulting: Statistical Analysis of Soil Data",
    "section": "",
    "text": "This consulting project was part of my graduate consulting curriculum, where students provide statistical consulting for the statistics department.\nThe client (Patrick Michelsen) was an Environmental Management M.S. student working on their thesis project. Patrick needed guidance on what statistical method to use for his randomized complete block design study with 6 response values. His main issue was he has many missing values and values under the limit of detection (LOD), aka left censored.\nMy group presented two approaches:\n\nUsing Bayesian Statistics with the brms library, which allowed us to use a mixed model with left censored data\nTreating the LOD values as 0 and using a Zero-Inflated model to separately model the LOD and actual values\n\nFor the two response variables without any LOD values, we presented generalized linear mixed models.\nGithub Repo\nReport"
  },
  {
    "objectID": "projects/2025-05-07-stat566-02/jacob_matties.html",
    "href": "projects/2025-05-07-stat566-02/jacob_matties.html",
    "title": "Graduate Consulting: Balance Detection using Phones",
    "section": "",
    "text": "This consulting project was part of my graduate consulting curriculum, where students provide statistical consulting for the statistics department.\nThe client (Jacob Matties) was a ME M.S. student working on their thesis project. Jacob needed help determining whether a smartphone placed in different locations (shirt pocket, pants pocket, handheld) could detect differences in balance between different stances (single leg, both legs, tandem).\nMy group used a generalized linear mixed model to detect this difference. In our report, we provided Jacob with instructions for how to perform this analysis in JMP as well as our findings and interpretations of the analysis. We checked the JMP output with SAS output as well to be thorough.\nSAS Code\nReport"
  },
  {
    "objectID": "projects/2025-03-18-stat417/stat417.html",
    "href": "projects/2025-03-18-stat417/stat417.html",
    "title": "STAT 417: Survival Analysis of Customer Churn",
    "section": "",
    "text": "The final project for this class had my group explore a dataset using survival analysis.\nWe decided to use the Telco Customer Churn dataset, which is a fictional dataset created by IBM to simulate customer data for a telecommunications company. This dataset was designed to help predict customer churn, referring to customers who stop using the company’s services, with the goal of analyzing customer behavior and developing strategies to retain customers.\nOur report is split into three sections: parametric survival analysis, non-parametric survival analysis, and cox regression. The parametric analysis was done in JMP, while the latter two approaches were done in R.\nGithub Repo\nReport"
  },
  {
    "objectID": "projects/2024-03-21-stat550/stat550.html",
    "href": "projects/2024-03-21-stat550/stat550.html",
    "title": "STAT 550: Generalized Linear Models",
    "section": "",
    "text": "For the final project of this class, I was given a dataset and asked to perform exploratory data analysis to find:\n\nFind a model that “best” explains the relationship between your response variable(s) and the predictor variables, then interpret those relationships\nMake predictions (either probabilities or counts—as appropriate) from your final model\n\nMy final model ended up being a negative binomial generalized linear model with a log link. The Project Log details the steps I took to get to this final model.\nGithub Repo\nReport\nProject Log"
  },
  {
    "objectID": "projects/2023-10-25-data403-01/iowa_liquor.html",
    "href": "projects/2023-10-25-data403-01/iowa_liquor.html",
    "title": "Data 403: Iowa Liquor Sales Analysis",
    "section": "",
    "text": "This project consisted of 2 practice clients to prepare us for real client projects. Alongside preparing reports for these clients, we gave in person presentations!\n\nClient A\nThe first client, Booze ‘R’ Us, requested a method by which to predict future sales for growth purposes. We proposed a machine learning approach fitting a multiple linear regression model to historical monthly sales data from Booze ‘R’ Us’s storefronts and applied the modeling process to a case study similar in scope. The model was fit to historical data and accurately predicted future monthly sales for an average storefront. Furthermore, the features included reflected factors that we found tend to drive or have the greatest impact on monthly liquor sales. Through our analysis, we selected three main feature setups and evaluated each to finalize using month, size of bottles, price, and type of alcohol. In addition to developing a robust predictive model, we identified specific sizes, price ranges, and liquor types that serve as the primary drivers of sales.\n\n\nClient B\nThe second client, Drinking Excess Alcohol is Dangerous (DEAD), was interested in the driving factors behind small and large alcohol purchases in Iowa. We proposed a machine learning model fitting a multiple linear regression model to a random sample of past sales data. The model fits sales data and predicts the quantity of alcohol purchased. The features of the final model represent the most influential factors driving alcohol purchasing behavior. This investigation has revealed valuable insights that can guide DEAD in shaping responsible alcohol retailing initiatives, with a focus on seasonal, category-specific, and college town-targeted campaigns, while upholding ethical principles of transparency and data privacy.\nGithub Repo\nClient A Report\nClient B Report\nSlides"
  },
  {
    "objectID": "projects/2023-03-16-csc369/final_project.html",
    "href": "projects/2023-03-16-csc369/final_project.html",
    "title": "CSC 369: Electric Vehicle Ownership",
    "section": "",
    "text": "We had the option of using Hadoop or Pyspark for this project since we learned both during the course, I chose to use Pyspark.\nFor the final project for this course, I investigated electric vehicle ownership within Washington as of February 17, 2023. My focus was to answer the following questions:\n\nHas there been an increase in electric vehicle ownership within the past few years?\nWhat make of vehicle was the most popular each year? Within each make, was there a stand out model?\nAre there more Full Electric or Hybrid vehicles currently registered? Do different cities favor one type over the other? Do counties? Do Postal Codes?\n\nAt the end of my analysis, I could conclude that over time there has been an increase in ownership of electric vehicles in Washington; with larger increases in the past two years. From 2011 to 2017, the Nissan Leaf was the most popular vehicle (I owned one of those!), and from 2018 to present day Tesla has held the reigns. Currently full electric vehicles are the most popular compared against hybrid vehicles, regardless of location. This follows what we saw from question 2 since Tesla, a full electric vehicle manufacturer, has been the dominate make for the past few years.\nGithub Repo\nData"
  },
  {
    "objectID": "projects/2022-03-13-csc203/virtual_world.html",
    "href": "projects/2022-03-13-csc203/virtual_world.html",
    "title": "CSC 203: Virtual World Project",
    "section": "",
    "text": "At the beginning of the Quarto we were given a single Java file with everything to run the game put into it (a non-object oriented style). Throughout the quarter, I applied different object-oriented programming techniques to this project.\n\nProject 1: Move almost all static methods in the file Functions.java to the appropriate class\nProject 2: Eliminate function duplication by refactoring the duplication into new abstract classes\nProject 3: Refactor the duplicate variables/methods into a hierarchy of abstract classes\nProject 4: Modify the pathing behavior of all entities that move within the world (A* Pathing)\nProject 5: Modify the virtual world to support a “world-changing” event. This event is to be triggered by a mouse press and must have a localized effect. The event must be visualized by changing the affected background tiles and by modifying the affected entities. In addition, the world event must create a new type of entity\n\nChanges made to the code were planned out beforehand in a UML using draw.io. I have multiple versions of my UML at each project point.\nGithub Repo"
  },
  {
    "objectID": "projects/2022-03-08-data301/essays.html",
    "href": "projects/2022-03-08-data301/essays.html",
    "title": "Data 301: Essay Autograder",
    "section": "",
    "text": "For the final project for this course, we applied the techniques learned in this class to analyze a data set of personal interest to you. My group decided to extend the ability to auto grade multiple choice and short response questions to essay questions, with a focus on sentiment analysis.\nWe acquired over 11k essays from the Hewlett Foundation on Kaggle written by students in grades 7 to 10 with 8 different prompts. Using the scores given to these essays along with a polarity value from a sentiment analysis API, we created two K-Nearest Neighbors models. One model predicted the essays grade based on the words in the essay (TF-IDF), and the other on the polarity score of the essay. Both models were run on each essay prompt individually as well as all essays at once.\nWhen comparing the two models, on average the predictions from the polarity models performed better than those from the TF/TF-IDF models. This supports our findings from our data exploration; that the polarity does affect the score.\nFrom the data we acquired, accounting for the grade of the writer, type of essay, the polarity, and the general polarity of the writing produces the best results. Furthermore, with the amount of samples we had, looking at all the essays overall provides better predictions than each prompt separately. With larger sample sizes of each prompt, we would expect the opposite conclusion since focusing on essays only from a single prompt results in less unexplained variability between the essays. Furthermore, we discovered that the polarity of an essay is correlated with different scores, as it proved to be a fairly good predictor on its own when compared to the TF-IDF model.\nGithub Repo\nReport"
  },
  {
    "objectID": "projects/2022-12-09-csc365/CSGO_app.html",
    "href": "projects/2022-12-09-csc365/CSGO_app.html",
    "title": "CSC 365: Esports Gambling Application",
    "section": "",
    "text": "The final project for this class was to build an application backed by a database that handles transactions. The application needed some form of user interface (e.g., command-line, website, etc.), and must be able to handle multiple users concurrently with a user-login system. The application has both a transactional side (for example if a hotel reservation system it should allow you to book hotel rooms) and a reporting side (for example, if a hotel reservation system, you should be able to get a report of vacancies over time). It must be able to support at least 3 different types of transactions and 3 different types of reporting queries, where the reporting queries allow multiple dimensional breakdowns selectable by the user. Lastly, the database has at least 100k rows of total data in it.\nMy group chose to create an esports team gambling app, specifically CSGO. Users can login and bet on specific aspects of each match between multiple different teams. This can be from which player will deal the most damage to which team will win to whether or not a certain amount of kills are met. Users are able to deposit money and withdraw money from their account.\nUsers have access to summary statistics of their bets and statistics from past matches which can be broken down by team, player, map, and year. Some statistics include money won/lost over time and bets placed on team/player by year. Admins can see the amount of profit made from each user, the amount of profit made by bets on a team/player in a year.\nUser and game data was saved in a relational-database style using sqlalchemy.\nGithub Repo\nData"
  },
  {
    "objectID": "projects/2023-06-14-stat545/stat545.html",
    "href": "projects/2023-06-14-stat545/stat545.html",
    "title": "STAT 545: Applied Stochastic Processes",
    "section": "",
    "text": "This class did not have a final project, however it had multiple challenging projects throughout the Quarter.\nGithub Repo\n\nSimple Random Walk\nThis project required me to write my own simple random walk function and use it to answer multiple questions\nProject 1 Report\n\n\nMarkov Chains\nThis project required me to write my own code to simulate markov chains and approximate distributions.\nProject 2 Report\n\n\nMore Markov Chains\nThis project required me to further explore markov chains, absorption states, and stationary distributions.\nProject 3 Report\n\n\nChutes and Ladders and MCMC\nThis project had me create a board of chutes and ladders, following specific set up rules. Throughout the project I explored time to absorption (how many turns it takes to get to the end of the board) and used simulation to determine the placement of each chute and ladder to minimize and maximize the time to absorption.\nProject 4 Report\n\n\nCostco Time Markov Chains\nThis project had me create a Costco gas station, fully equipped with multiple islands, pumps, and wait times. I simulated a multitude of items such as the distribution of number of cars in the system, fraction of time there are no cars in the system, and customer wait time. I created different types of cars (i.e. will only join queues on the right) using setClass for easy tracking!\nProject 5 Report"
  },
  {
    "objectID": "projects/2023-11-17-data403-02/home_credit.html",
    "href": "projects/2023-11-17-data403-02/home_credit.html",
    "title": "Data 403: Home Credit Default Risk",
    "section": "",
    "text": "This project consisted of a practice clients to prepare us for real client projects. Alongside preparing reports for this client, we gave an in person presentation!\nThe overview we were provided was:\nYou are a data scientist at a bank working in the mortgages division. You have been tasked with building a model to predict whether an applicant will be able to repay their loan. Use the data from this Kaggle competition, specifically the data in application_train.csv, to build and evaluate your model.\nFurthermore, we were limited to implementing the machine learning algorithms, metrics, and cross-validation process from scratch.\nMy group performed the following tasks and detailed our decisions and methods in our report:\n\nThe first section, “Data Collection and Preparation,” includes a description of the data set, cleaning and preprocessing steps, and feature selection rationale.\nThe second section, “Model Selection and Validation,” includes an overview of the models: logistic regression, support vector machines, and linear discriminant analysis, along with criteria for model comparison and selection.\nThe third section, “Final Model,” provides a detailed description and justification of the chosen final model.\nThe fourth section, “Ethical Concerns,” includes a summary of potential biases in the model and recommendations for ethical use.\nThe fifth section, “Conclusion,” summarizes our key findings and their significance.\n\nGithub Repo\nReport\nSlides"
  },
  {
    "objectID": "projects/2024-06-12-data451/world_bank.html",
    "href": "projects/2024-06-12-data451/world_bank.html",
    "title": "Data Science Capstone: World Bank Digital Development Classification Project",
    "section": "",
    "text": "This data science capstone project developed an automated system to classify World Bank projects and summarize their activities, specifically focusing on broadband connectivity. The project addressed the challenge of encapsulating the World Bank’s impact due to the complexity and quantity of its projects.\nThe solution involved a three-step process:\n\nClassification: Two approaches, a Naïve approach and a Large Language Model (LLM) approach (using BERT and Llama models), were employed to classify projects based on whether they relate to broadband connectivity by analyzing indicator names.\nFiltering: Classified indicators were refined to ensure only the most up-to-date observations were kept and that ‘Progress Values’ were numerical.\nAggregation: Progress for each indicator was computed and converted to counts where necessary.\n\nThe project compared the Naïve, BERT, and Llama models, identifying their strengths and weaknesses in classification. Future steps include a hybrid classification approach combining BERT and TF-IDF, refining filtering and aggregation methods for LLM outputs, and leveraging pre-labeled data from the World Bank for enhanced model training and evaluation. Additionally, the BART-large-mnli model showed promise towards the end of the project, and could further enhance classification capabilities.\nDue to an NDA, I can not share the github repository or data.\nReport\nPoster\nSlides"
  },
  {
    "objectID": "projects/2025-05-02-stat566-01/elsa_bates.html",
    "href": "projects/2025-05-02-stat566-01/elsa_bates.html",
    "title": "Graduate Consulting: Spinal Segment Curve Analysis",
    "section": "",
    "text": "This consulting project was part of my graduate consulting curriculum, where students provide statistical consulting for the statistics department.\nThe client (Elsa Bates) was a BMED M.S. student working on their thesis project involving using wearable posture-monitoring sensors (IMUs) to quantify spinal angles, and compare these measurements to the gold standard motion capture system sensors. Elsa needed help determining whether the IMU data is “close enough” to the motion capture data, and how to best summarise and present these findings across all 30 participants in her results section.\nIn summary, my group did the following:\n\nData Management (combine files, align times, calculate curve characteristics)\nCompare curves wholistically (visually, correlation coefficients, dynamic time warping)\nCompare characteristics of curves using linear mixed models (e.g., proportion of time spent in harmful posture, range of motion, etc.)\n\nGithub Repo\nReport"
  },
  {
    "objectID": "projects/2025-06-06-stat415/stat415.html",
    "href": "projects/2025-06-06-stat415/stat415.html",
    "title": "STAT 415: Bayesian Survival Analysis of Customer Churn",
    "section": "",
    "text": "The final project for this class had my group explore a past Frequentist Analysis in a Bayesian light. Since I worked with the same person for my Survival Analysis final project, we decided to use this analysis.\nWe implement a Bayesian Weibull proportional hazards model. The Weibull distribution was chosen based on the parametric survival analysis from our Stat 417 report, which identified it as the best-fitting distribution (Anderson-Darling test statistic = 16986.795). We will use the same predictors in the Bayesian model as we did in our frequentist model to enable a direct comparison between the two.\nThe Bayesian and frequentist approaches yielded similar estimates for key effects:\n\nContract effects differ by less than 2%\nPayment method effects are consistent in direction and similar in magnitude\nBoth identify the same key predictors of churn\n\nThe main differences are:\n\nThe Bayesian approach doesn’t suffer from proportional hazards violations\nWe get full posterior distributions rather than point estimates\n\nGithub Repo\nReport"
  },
  {
    "objectID": "projects/2025-06-11-stat566-03/noah_jeffery.html",
    "href": "projects/2025-06-11-stat566-03/noah_jeffery.html",
    "title": "Graduate Consulting: Spinal Segment Angle Analysis",
    "section": "",
    "text": "This consulting project was part of my graduate consulting curriculum, where students provide statistical consulting for the statistics department.\nThe client (Noah Jeffery) was a Biomedical Engineering M.S. student working on their thesis project. Noah had come to the consulting department a few weeks earlier to help design his study, and has returned with collected data. He put three devices on his subjects to measure the angle of their spine while sitting, with the goal of showing that all three devices are necessary to measure different postures.\nThe first part of his study was 15 minutes of unstructured time. From this data he wanted summary statistics. The second part of the study had the subjects following a guided video where they would perform multiple postures (good, laying back, lumbar slouch, etc.). From this data he wanted what angle thresholds and combinations that resulted in poor posture (anything that wasn’t good posture) and to mark these postures in the 15 minutes of unstructured time.\nMy group did the following:\n\nUsed linear mixed models to determine the confidence and prediction intervals for each posture for each device in the structured time (guided video section)\nCreated visualizations showing that each device was able to identify different postures, requiring all three devices to detect all postures\nApplied the prediction intervals to the 15 minutes of unstructured time to create plots for each subject, as well as summary statistics across all subjects (e.g. time spent in each posture)\n\nGithub Repo\nReport"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "California Polytechnic State University, San Luis Obispo\n     M.S. Statistics, Minor in Data Science\nGraduated: June 2025\n     GPA: 3.8 / 4.0"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "",
    "text": "California Polytechnic State University, San Luis Obispo\n     M.S. Statistics, Minor in Data Science\nGraduated: June 2025\n     GPA: 3.8 / 4.0"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Resume",
    "section": "Experience",
    "text": "Experience\n\nStatistical Consultant\nDepartment of Statistics Cal Poly San Luis Obispo\nApril 2025 - June 2025\n\nProvided statistical guidance to university faculty and students on complex research designs and data analysis challenges\nCollaborated with clients and other consultants to define research questions, determine plausible analytical approaches, and interpret findings\nConducted in-depth data analysis, addressing issues such as high numbers of zeros and non-normal data distributions\nCommunicated complex statistical methodologies and results effectively to non-statistical audiences, translating data into actionable knowledge\n\n\n\nStudent Researcher for DL4S&R\nCal Poly San Luis Obispo\nFebruary 2023 - June 2025\n\nPerform exploratory data analysis on thousands of search and rescue missions\nInvestigate machine learning and deep learning methodologies to enhance training and evaluation of a multimodal model\nDevelop scripts using R and Python to perform data cleaning while documenting modifications and findings using Quarto and Jupyter notebooks\n\n\n\nUndergraduate Research Fellow\nDepartment of Statistics Cal Poly San Luis Obispo\nJanuary 2022 - August 2022\n\nDeveloped the largest national database of sexual misconduct cases (20k records) between K-12 educators through data collection and database merging\nComputed descriptive statistics and visualizations using Python and R for research paper\nCleaned collected and outside data, then merged 6 databases together while avoiding duplicate observations"
  },
  {
    "objectID": "resume.html#projects",
    "href": "resume.html#projects",
    "title": "Resume",
    "section": "Projects",
    "text": "Projects\n\nThesis\nSeptember 2024 - June 2025\nFrequent Itemset Mining with tidyclust in R\n\nContributed implementations for Apriori and ECLAT algorithms to the tidyclust package in R, expanding the package to include frequent itemset mining\nIntegrated a novel clustering approach that groups column variables based on their “dominant” frequent itemsets\nDesigned and implemented a new prediction methodology framed as a recommender system to predict missing items in transactional datasets, utilizing confidence from frequent itemsets\nCreated new functions, extract_predictions() and augment_itemset_predict(), to standardize and prepare frequent itemset prediction outputs for user-friendly analysis and evaluation within the tidymodels framework\n\n\n\nTFT API\nSeptember 2023 - Present\n\nUtilized Riot Games API to scrape and clean 10,000+ TFT match histories, implementing custom Python functions for efficient data extraction and preprocessing\nDesigned a relational database using SQLAlchemy and Supabase to store and query match data\nApplied multilevel modeling in R to analyze player performance trends, identifying key factors correlated with win rates\nBuilt a Shiny web app to recommend data-driven strategies based on player inputs\n\n\n\nWorld Bank Digital Development Classification Project\nStudent Consultant\nJanuary 2024 - June 2024\n\nDeveloped and compared two classification methods: a rule-based keyword approach and machine learning models (BERT, LLAMA, BART)\nImplemented filtering and aggregation methods to track project progress and measure impact on broadband connectivity across countries\nProposed a hybrid classification model combining TF-IDF and contextual embeddings for enhanced accuracy"
  },
  {
    "objectID": "resume.html#programming-languages",
    "href": "resume.html#programming-languages",
    "title": "Resume",
    "section": "Programming Languages",
    "text": "Programming Languages\n\nJava\nJMP\nMinitab\nPython\nR\nSQL\nSAS"
  },
  {
    "objectID": "resume.html#publications",
    "href": "resume.html#publications",
    "title": "Resume",
    "section": "Publications",
    "text": "Publications\n\nTaking the Next Step in Exploring the Literary Digest 1936 Poll"
  },
  {
    "objectID": "resume.html#certifications-and-awards",
    "href": "resume.html#certifications-and-awards",
    "title": "Resume",
    "section": "Certifications and Awards",
    "text": "Certifications and Awards\n\nNational Statistics Honarary Society Member\nMay 2023\nMu Sigma Rho\n\n\nSAS Certified Specialist: Base Programming Using SAS 9.4\nMarch 2022\nSAS Certification"
  }
]